{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score #model evaluation\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
    "from sklearn.model_selection import cross_val_score # cross validation metrics\n",
    "from sklearn.model_selection import cross_val_predict # cross validation metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why K-Fold?\n",
    "\n",
    "In the lecture we learned about 3 types of model validation: *Train-Test-Split*, *K-Fold*, and *Leave-One-Out* (which is just an extreme version of K-Fold).\n",
    "\n",
    "TTS is easy, and computationally inexpensive, so why use K-Fold? One reason we discussed is that K-Fold allows you to use ALL your data in the test-set, and all your data in the training-set at (at least) one point.\n",
    "\n",
    "Use the simulation below to look at how well TTS vs KF estimate the out-of-sample (test-set) error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelValidationSim(n = 100):\n",
    "    \n",
    "    # Simulate Data---------------------------------------------------------------\n",
    "    \n",
    "    error_sd = 1\n",
    "    # mean cat length in cm\n",
    "    mean_length_cm = 170\n",
    "    # standard deviation of cat length in cm\n",
    "    sd_length_cm = 15\n",
    "    \n",
    "    # generate random data for cat length that follows a normal distribution\n",
    "    length = np.random.normal(loc = mean_length_cm, scale = sd_length_cm, size = n)\n",
    "    \n",
    "    # weight = intercept + length*coefficient + random error\n",
    "    weight = 0.2 + length*0.04 + np.random.normal(0,error_sd,n)\n",
    "    \n",
    "    \n",
    "    cats = pd.DataFrame({\"length\": length, \"weight\": weight})\n",
    "    \n",
    "\n",
    "    features = [\"length\"]\n",
    "    X = cats[features]\n",
    "    y = cats[[\"weight\"]] #if you don't have the extra brackets, y will be a series instead of an array and throw an error\n",
    "    \n",
    "    ##############################################\n",
    "    # Build a model using a Train Test Split with 20% (1/5th) of data in the test set\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store the test-set MSE in the variable TTS\n",
    "    \n",
    "    TTS = ###\n",
    "\n",
    "    ##############################################\n",
    "    # Build a model using a 5-Fold CV\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store the average test-set MSE in the variable KF\n",
    "    \n",
    "    KF = ###\n",
    "    \n",
    "    ##############################################\n",
    "    \n",
    "    # Return a dataframe with the KFold as one Column, and TrainTestSplit as the other (the df should have just 1 row)\n",
    "    # (hint: If using pd.DataFrame() and a dictionary to create the df, make sure you put KF and TTS into\n",
    "    # a list before setting them as a values for your dictionary)\n",
    "    \n",
    "    ##############################################\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question*\n",
    "\n",
    "* In the cell below, use list comprehension to run this simulation 500 times.\n",
    "* Then use ggplot to plot the histograms (or I highly recommend densities `geom_density(alpha = 0.4)` if you wanna get fancy) of the estimated MSE's from TTS vs KFold (look up `pd.melt()` to get your data in long form for this one)\n",
    "* Plot a scatterplot of the TTS vs KFold estimates\n",
    "* What patterns do you see? Is TTS systematically different from KFold? How could any differences you see affect your interpretation of TTS vs KF, or affect which you choose to use?\n",
    "* Try changing `error_sd` (the standard deviation of the random error we add to the simulated data. The larger `error_sd` is, the more spread out data will be around the regression line), does this change the mean squared error estimates? Is there a certain pattern you can see?\n",
    "* change `n` (the number of samples) to be 1000. Does this change anything? Change the difference between KF and TTS?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = 200px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
