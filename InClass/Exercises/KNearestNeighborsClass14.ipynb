{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Together\n",
    "\n",
    "## Overview\n",
    "KNN is a simple, distance based algorithm that let's us CLASSIFY data points based on what class the data points around them are. Birds of a feather...\n",
    "\n",
    "Despite it being distance based, KNN is a *classification* algorithm. In other words, it is supervised machine learning, as it requires truth labels (the actual class/group). However it does share characteristics with clustering algorithms we will see later.\n",
    "\n",
    "KNN *can* work with binary/categorical variables, but not without some tweaking which we do not cover here.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters are parameters in our model that are NOT chosen by the algorithm (we must supply them). We can either choose them:\n",
    "\n",
    "- based on domain expertise (knowledge about the data)\n",
    "- based on the data (hyperparameter tuning)\n",
    "\n",
    "Why do we have to use a validation set when hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "Use the telecom_churn.csv data from GitHub (for more information see this [link](https://www.kaggle.com/ivanhrek/telecom-churn)) and the KNN algorithm to predict `churn` in this dataset. Use TTS, and only continuous/interval variables. Z score your variables, and use `GridSearchCV()` to choose `k`.\n",
    "\n",
    "How accurate is your model? Is it just as good at predicting people who do not churn, as people who do churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN From Scratch\n",
    "\n",
    "Write a function, `neighbors()` that takes in three arguments:\n",
    "\n",
    "- `k`: the number of neighbors to find\n",
    "- `df`: a dataframe with ONLY continuous variables (can be any # of rows or columns)\n",
    "- `point`: the values of the data point you're finding neighbors for\n",
    "\n",
    "This function should find the euclidean distance between `point` and every other data point in `df` (hint: `np.linalg.norm()`, and return a list of the indices of the `k` nearest neighbors (by indices, I mean that if the k-nearest neighbors are in the 0th, 15th, 23rd, 32nd, and 56th rows, you should return a list `[0,15,23,32,56]`). Assume that the datapoint `point` is NOT included as a row in `df`.\n",
    "\n",
    "You may use `np.argpartition()` to find the indices of the `k` nearest neighbors. Below is an example of how it works. [Documentation](https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html) linked here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an array of numbers\n",
    "ar = [1, 7, 9, 2, 0.1, 17, 17, 1.5]\n",
    "\n",
    "# k (# of items)\n",
    "k = 3\n",
    "\n",
    "# get indices of k smallest values in ar\n",
    "indices = np.argpartition(ar, k)[:k]\n",
    "\n",
    "# check if this is correct\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "d = pd.DataFrame({\"x\" : [1.449, -1.069, -0.855, -0.281, -0.994, -0.969, -1.107, -1.252, -0.524, -0.497],\n",
    "                  \"y\" : [-1.806, -0.582, -1.109, -1.015, -0.162,  0.563,  1.648, -0.773,  1.606, -1.158]})\n",
    "\n",
    "p = np.array([-0.75, -1])\n",
    "\n",
    "k = 3\n",
    "\n",
    "# if your function is working, this should output `True`\n",
    "set(neighbors(k,d,p)) == set(np.array([3,2,9]))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
